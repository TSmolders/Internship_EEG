{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model: graphLambda without edge_attr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightning.pytorch as pl\n",
    "from torchmetrics import F1Score, ConfusionMatrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch_geometric\n",
    "from torch.nn import Sequential, Linear, ReLU, BatchNorm1d, Dropout, Dropout1d, ModuleList\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import  GCNConv, global_add_pool, GATConv, GINConv\n",
    "\n",
    "\n",
    "n_features = 25 # [Delta, Theta, Alpha, Beta, Gamma] * [mean, median, std, skew, kurt]\n",
    "num_classes = 5 # healthy, ADHD, MDD, OCD, SMC\n",
    "\n",
    "class graphLambda_wo_edge_attr(pl.LightningModule):\n",
    "    def __init__(self, learning_rate, optimizer_name, fc_layers, dropout):\n",
    "        super(graphLambda_wo_edge_attr, self).__init__()\n",
    "        self.learning_rate = learning_rate\n",
    "        self.optimizer_name = optimizer_name\n",
    "\n",
    "        #GCN-representation\n",
    "        self.conv1 = GCNConv(n_features, 256, cached=False )\n",
    "        self.bn01 = BatchNorm1d(256)\n",
    "        self.conv2 = GCNConv(256, 128, cached=False )\n",
    "        self.bn02 = BatchNorm1d(128)\n",
    "        self.conv3 = GCNConv(128, 128, cached=False)\n",
    "        self.bn03 = BatchNorm1d(128)\n",
    "        #GAT-representation\n",
    "        self.gat1 = GATConv(n_features, 256, heads=3)\n",
    "        self.bn11 = BatchNorm1d(256*3)\n",
    "        self.gat2 = GATConv(256*3, 128, heads=3)\n",
    "        self.bn12 = BatchNorm1d(128*3)\n",
    "        self.gat3 = GATConv(128*3, 128, heads=3)\n",
    "        self.bn13 = BatchNorm1d(128*3)\n",
    "        #GIN-representation\n",
    "        fc_gin1=Sequential(Linear(n_features, 256), ReLU(), Linear(256, 256))\n",
    "        self.gin1 = GINConv(fc_gin1)\n",
    "        self.bn21 = BatchNorm1d(256)\n",
    "        fc_gin2=Sequential(Linear(256, 128), ReLU(), Linear(128, 128))\n",
    "        self.gin2 = GINConv(fc_gin2)\n",
    "        self.bn22 = BatchNorm1d(128)\n",
    "        fc_gin3=Sequential(Linear(128, 64), ReLU(), Linear(64, 64))\n",
    "        self.gin3 = GINConv(fc_gin3)\n",
    "        self.bn23 = BatchNorm1d(64)\n",
    "        # Fully connected layers for concatinating outputs (varied depending on fc_layers)\n",
    "        self.fcs = ModuleList()\n",
    "        self.dropouts = ModuleList()\n",
    "        input_size = 128*4 + 64 # Input size for the first layer\n",
    "        output_size = num_classes # Output size for the last layer\n",
    "        sizes = np.linspace(input_size, output_size, fc_layers + 1, dtype=int) # Calculate the size for each layer\n",
    "        for i in range(fc_layers): # Create the layers\n",
    "            self.fcs.append(Linear(sizes[i], sizes[i + 1]))\n",
    "            self.dropouts.append(Dropout(p=dropout))\n",
    "\n",
    "        # add metrics\n",
    "        self.train_f1 = F1Score(task='multiclass', num_classes=num_classes, average='macro')\n",
    "        self.val_f1 = F1Score(task='multiclass', num_classes=num_classes, average='macro')\n",
    "        self.val_cm = ConfusionMatrix(task = 'binary', num_classes=num_classes, threshold=0.05)\n",
    "        self.validation_step_yhats = []\n",
    "        self.validation_step_ys = []\n",
    "        \n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        y=x\n",
    "        z=x\n",
    "        #GCN-representation\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        x = self.bn01(x)\n",
    "        x = F.relu(self.conv2(x, edge_index))\n",
    "        x = self.bn02(x)\n",
    "        x = F.relu(self.conv3(x, edge_index))\n",
    "        x = self.bn03(x)\n",
    "        x = global_add_pool(x, data.batch)\n",
    "        #GAT-representation\n",
    "        y = F.relu(self.gat1(y, edge_index))\n",
    "        y = self.bn11(y)\n",
    "        y = F.relu(self.gat2(y, edge_index))\n",
    "        y = self.bn12(y)\n",
    "        y = F.relu(self.gat3(y, edge_index))\n",
    "        y = self.bn13(y)\n",
    "        y = global_add_pool(y, data.batch)\n",
    "        #GIN-representation\n",
    "        z = F.relu(self.gin1(z, edge_index))\n",
    "        z = self.bn21(z)\n",
    "        z = F.relu(self.gin2(z, edge_index))\n",
    "        z = self.bn22(z)\n",
    "        z = F.relu(self.gin3(z, edge_index))\n",
    "        z = self.bn23(z)\n",
    "        z = global_add_pool(z, data.batch)\n",
    "        #Concatinating_representations\n",
    "        cr=torch.cat((x,y,z),1)\n",
    "        for fc, dropout in zip(self.fcs, self.dropouts):\n",
    "            cr = F.relu(fc(cr))\n",
    "            cr = dropout(cr)\n",
    "        #cr = F.relu(cr).view(-1)\n",
    "        cr = F.log_softmax(cr, dim=1) # Activation function for classification\n",
    "        return cr  \n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        if self.optimizer_name == 'Adam':\n",
    "            optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n",
    "        elif self.optimizer_name == 'RMSprop':\n",
    "            optimizer = torch.optim.RMSprop(self.parameters(), lr=self.learning_rate)\n",
    "        elif self.optimizer_name == 'SGD':\n",
    "            optimizer = torch.optim.SGD(self.parameters(), lr=self.learning_rate)\n",
    "        else:\n",
    "            raise ValueError(f'Unsupported optimizer: {self.optimizer_name}')\n",
    "        return optimizer\n",
    "    \n",
    "    def training_step(self, train_batch, batch_idx):\n",
    "        x, y = train_batch.x, train_batch.y\n",
    "        output = self.forward(train_batch)\n",
    "        output = output.to(torch.float)\n",
    "        y = y.to(torch.long)\n",
    "        loss = F.nll_loss(output, y)\n",
    "        y_hat = output.argmax(dim=1)\n",
    "        self.log('train_loss', loss, on_epoch=True, prog_bar=True)\n",
    "        self.log('train_f1', self.train_f1(y, y_hat), on_epoch=True, prog_bar=True)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, val_batch, batch_idx):\n",
    "        x, y = val_batch.x, val_batch.y\n",
    "        output = self.forward(val_batch)\n",
    "        output = output.to(torch.float)\n",
    "        y = y.to(torch.long)\n",
    "        loss = F.nll_loss(output, y)\n",
    "        y_hat = output.argmax(dim=1)\n",
    "        self.log('val_loss', loss, on_epoch=True, prog_bar=True)\n",
    "        self.log('val_f1', self.val_f1(y, y_hat), on_epoch=True, prog_bar=True)\n",
    "        self.validation_step_yhats.append(y_hat)\n",
    "        self.validation_step_ys.append(y)\n",
    "        return loss\n",
    "    \n",
    "    def test_step(self, test_batch, batch_idx):\n",
    "        x, y = test_batch.x, test_batch.y\n",
    "        output = self.forward(test_batch)\n",
    "        output = output.to(torch.float)\n",
    "        y = y.to(torch.long)\n",
    "        loss = F.nll_loss(output, y)\n",
    "        y_hat = output.argmax(dim=1)\n",
    "        self.log('test_loss', loss, on_epoch=True, prog_bar=True)\n",
    "        self.log('test_f1', self.val_f1(y, y_hat), on_epoch=True, prog_bar=True)\n",
    "        return loss\n",
    "    \n",
    "    # def on_train_epoch_end(self, training_step_outputs):\n",
    "    #     # compute metrics\n",
    "    #     train_f1 = self.train_f1.compute()\n",
    "    #     # log metrics\n",
    "    #     self.log(\"epoch_train_f1\", train_f1)\n",
    "    #     # reset all metrics\n",
    "    #     self.train_f1.reset()\n",
    "    #     print(f\"\\nf1: {train_f1:.4}\")\n",
    "\n",
    "    # def on_validation_epoch_end(self):\n",
    "    #     # plot confusion matrix\n",
    "    #     y_hat = torch.cat(self.validation_step_yhats)\n",
    "    #     y = torch.cat(self.validation_step_ys)\n",
    "    #     confusion_matrix = self.val_cm(y_hat, y.int())\n",
    "\n",
    "    #     confusion_matrix_computed = confusion_matrix.detach().cpu().numpy().astype(int)\n",
    "\n",
    "    #     df_cm = pd.DataFrame(confusion_matrix_computed)\n",
    "    #     plt.figure(figsize = (10,7))\n",
    "    #     fig_ = sns.heatmap(df_cm, annot=True, cmap='Spectral').get_figure()\n",
    "    #     plt.close(fig_)\n",
    "    #     self.loggers[0].experiment.add_figure(\"Confusion matrix\", fig_, self.current_epoch)\n",
    "\n",
    "    #     self.validation_step_yhats.clear() # free memory\n",
    "    #     self.validation_step_ys.clear()\n",
    "    \n",
    "    def predict_step(self, batch):\n",
    "        x, y = batch.x, batch.y\n",
    "        output = self.forward(batch)\n",
    "        loss = F.nll_loss(output, y)\n",
    "        y_hat = output.argmax(dim=1)\n",
    "        return y_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model: graphLambda with edge_attr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model\n",
    "import lightning.pytorch as pl\n",
    "from torchmetrics import F1Score, ConfusionMatrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch_geometric\n",
    "from torch.nn import Sequential, Linear, ReLU, BatchNorm1d, Dropout, Dropout1d, ModuleList\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import  GCNConv, global_add_pool, GATConv, GINConv, GATv2Conv\n",
    "\n",
    "\n",
    "n_features = 25 # [Delta, Theta, Alpha, Beta, Gamma] * [mean, median, std, skew, kurt]\n",
    "num_classes = 5 # healthy, ADHD, MDD, OCD, SMC\n",
    "n_edge_attrs = 5 # [Delta, Theta, Alpha, Beta, Gamma]\n",
    "\n",
    "class graphLambda_w_edge_attr(pl.LightningModule):\n",
    "    def __init__(self, learning_rate, optimizer_name, fc_layers, dropout):\n",
    "        super(graphLambda_w_edge_attr, self).__init__()\n",
    "        self.learning_rate = learning_rate\n",
    "        self.optimizer_name = optimizer_name\n",
    "\n",
    "        #GCN-representation\n",
    "        self.conv1 = GCNConv(n_features, 256, cached=False )\n",
    "        self.bn01 = BatchNorm1d(256)\n",
    "        self.conv2 = GCNConv(256, 128, cached=False )\n",
    "        self.bn02 = BatchNorm1d(128)\n",
    "        self.conv3 = GCNConv(128, 128, cached=False)\n",
    "        self.bn03 = BatchNorm1d(128)\n",
    "        #GAT-representation\n",
    "        self.gat1 = GATv2Conv(n_features, 256, heads=3, edge_dim=n_edge_attrs)\n",
    "        self.bn11 = BatchNorm1d(256*3)\n",
    "        self.gat2 = GATv2Conv(256*3, 128, heads=3, edge_dim=n_edge_attrs)\n",
    "        self.bn12 = BatchNorm1d(128*3)\n",
    "        self.gat3 = GATv2Conv(128*3, 128, heads=3, edge_dim=n_edge_attrs)\n",
    "        self.bn13 = BatchNorm1d(128*3)\n",
    "        #GIN-representation\n",
    "        fc_gin1=Sequential(Linear(n_features, 256), ReLU(), Linear(256, 256))\n",
    "        self.gin1 = GINConv(fc_gin1)\n",
    "        self.bn21 = BatchNorm1d(256)\n",
    "        fc_gin2=Sequential(Linear(256, 128), ReLU(), Linear(128, 128))\n",
    "        self.gin2 = GINConv(fc_gin2)\n",
    "        self.bn22 = BatchNorm1d(128)\n",
    "        fc_gin3=Sequential(Linear(128, 64), ReLU(), Linear(64, 64))\n",
    "        self.gin3 = GINConv(fc_gin3)\n",
    "        self.bn23 = BatchNorm1d(64)\n",
    "        # Fully connected layers for concatinating outputs (varied depending on fc_layers)\n",
    "        self.fcs = ModuleList()\n",
    "        self.dropouts = ModuleList()\n",
    "        input_size = 128*4 + 64 # Input size for the first layer\n",
    "        output_size = num_classes # Output size for the last layer\n",
    "        sizes = np.linspace(input_size, output_size, fc_layers + 1, dtype=int) # Calculate the size for each layer\n",
    "        for i in range(fc_layers): # Create the layers\n",
    "            self.fcs.append(Linear(sizes[i], sizes[i + 1]))\n",
    "            self.dropouts.append(Dropout(p=dropout))\n",
    "\n",
    "        # add metrics\n",
    "        self.train_f1 = F1Score(task='multiclass', num_classes=num_classes, average='macro')\n",
    "        self.val_f1 = F1Score(task='multiclass', num_classes=num_classes, average='macro')\n",
    "        self.val_cm = ConfusionMatrix(task = 'binary', num_classes=num_classes, threshold=0.05)\n",
    "        self.validation_step_yhats = []\n",
    "        self.validation_step_ys = []\n",
    "        \n",
    "    def forward(self, data):\n",
    "        x, edge_index, edge_attr = data.x, data.edge_index, data.edge_attr\n",
    "        y=x\n",
    "        z=x\n",
    "        #GCN-representation\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        x = self.bn01(x)\n",
    "        x = F.relu(self.conv2(x, edge_index))\n",
    "        x = self.bn02(x)\n",
    "        x = F.relu(self.conv3(x, edge_index))\n",
    "        x = self.bn03(x)\n",
    "        x = global_add_pool(x, data.batch)\n",
    "        #GAT-representation\n",
    "        y = F.relu(self.gat1(y, edge_index, edge_attr))\n",
    "        y = self.bn11(y)\n",
    "        y = F.relu(self.gat2(y, edge_index, edge_attr))\n",
    "        y = self.bn12(y)\n",
    "        y = F.relu(self.gat3(y, edge_index, edge_attr))\n",
    "        y = self.bn13(y)\n",
    "        y = global_add_pool(y, data.batch)\n",
    "        #GIN-representation\n",
    "        z = F.relu(self.gin1(z, edge_index))\n",
    "        z = self.bn21(z)\n",
    "        z = F.relu(self.gin2(z, edge_index))\n",
    "        z = self.bn22(z)\n",
    "        z = F.relu(self.gin3(z, edge_index))\n",
    "        z = self.bn23(z)\n",
    "        z = global_add_pool(z, data.batch)\n",
    "        #Concatinating_representations\n",
    "        cr=torch.cat((x,y,z),1)\n",
    "        for fc, dropout in zip(self.fcs, self.dropouts):\n",
    "            cr = F.relu(fc(cr))\n",
    "            cr = dropout(cr)\n",
    "        #cr = F.relu(cr).view(-1)\n",
    "        cr = F.log_softmax(cr, dim=1) # Activation function for classification\n",
    "        return cr  \n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        if self.optimizer_name == 'Adam':\n",
    "            optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n",
    "        elif self.optimizer_name == 'RMSprop':\n",
    "            optimizer = torch.optim.RMSprop(self.parameters(), lr=self.learning_rate)\n",
    "        elif self.optimizer_name == 'SGD':\n",
    "            optimizer = torch.optim.SGD(self.parameters(), lr=self.learning_rate)\n",
    "        else:\n",
    "            raise ValueError(f'Unsupported optimizer: {self.optimizer_name}')\n",
    "        return optimizer\n",
    "    \n",
    "    def training_step(self, train_batch, batch_idx):\n",
    "        x, y = train_batch.x, train_batch.y\n",
    "        output = self.forward(train_batch)\n",
    "        output = output.to(torch.float)\n",
    "        y = y.to(torch.long)\n",
    "        loss = F.nll_loss(output, y)\n",
    "        y_hat = output.argmax(dim=1)\n",
    "        self.log('train_loss', loss, on_epoch=True, prog_bar=True)\n",
    "        self.log('train_f1', self.train_f1(y, y_hat), on_epoch=True, prog_bar=True)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, val_batch, batch_idx):\n",
    "        x, y = val_batch.x, val_batch.y\n",
    "        output = self.forward(val_batch)\n",
    "        output = output.to(torch.float)\n",
    "        y = y.to(torch.long)\n",
    "        loss = F.nll_loss(output, y)\n",
    "        y_hat = output.argmax(dim=1)\n",
    "        self.log('val_loss', loss, on_epoch=True, prog_bar=True)\n",
    "        self.log('val_f1', self.val_f1(y, y_hat), on_epoch=True, prog_bar=True)\n",
    "        self.validation_step_yhats.append(y_hat)\n",
    "        self.validation_step_ys.append(y)\n",
    "        return loss\n",
    "    \n",
    "    def test_step(self, test_batch, batch_idx):\n",
    "        x, y = test_batch.x, test_batch.y\n",
    "        output = self.forward(test_batch)\n",
    "        output = output.to(torch.float)\n",
    "        y = y.to(torch.long)\n",
    "        loss = F.nll_loss(output, y)\n",
    "        y_hat = output.argmax(dim=1)\n",
    "        self.log('test_loss', loss, on_epoch=True, prog_bar=True)\n",
    "        self.log('test_f1', self.val_f1(y, y_hat), on_epoch=True, prog_bar=True)\n",
    "        return loss\n",
    "    \n",
    "    # def on_train_epoch_end(self, training_step_outputs):\n",
    "    #     # compute metrics\n",
    "    #     train_f1 = self.train_f1.compute()\n",
    "    #     # log metrics\n",
    "    #     self.log(\"epoch_train_f1\", train_f1)\n",
    "    #     # reset all metrics\n",
    "    #     self.train_f1.reset()\n",
    "    #     print(f\"\\nf1: {train_f1:.4}\")\n",
    "\n",
    "    # def on_validation_epoch_end(self):\n",
    "    #     # plot confusion matrix\n",
    "    #     y_hat = torch.cat(self.validation_step_yhats)\n",
    "    #     y = torch.cat(self.validation_step_ys)\n",
    "    #     confusion_matrix = self.val_cm(y_hat, y.int())\n",
    "\n",
    "    #     confusion_matrix_computed = confusion_matrix.detach().cpu().numpy().astype(int)\n",
    "\n",
    "    #     df_cm = pd.DataFrame(confusion_matrix_computed)\n",
    "    #     plt.figure(figsize = (10,7))\n",
    "    #     fig_ = sns.heatmap(df_cm, annot=True, cmap='Spectral').get_figure()\n",
    "    #     plt.close(fig_)\n",
    "    #     self.loggers[0].experiment.add_figure(\"Confusion matrix\", fig_, self.current_epoch)\n",
    "\n",
    "    #     self.validation_step_yhats.clear() # free memory\n",
    "    #     self.validation_step_ys.clear()\n",
    "    \n",
    "    def predict_step(self, batch):\n",
    "        x, y = batch.x, batch.y\n",
    "        output = self.forward(batch)\n",
    "        loss = F.nll_loss(output, y)\n",
    "        y_hat = output.argmax(dim=1)\n",
    "        return y_hat"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
